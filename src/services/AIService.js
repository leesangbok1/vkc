// AI ì„œë¹„ìŠ¤ - OpenAI API í†µí•©
class AIService {
  constructor() {
    this.apiKey = import.meta.env.VITE_OPENAI_API_KEY || ''
    this.apiUrl = 'https://api.openai.com/v1'
    this.model = 'gpt-3.5-turbo'
    this.isEnabled = !!this.apiKey

    // ìš”ì²­ ìºì‹œ (ë™ì¼í•œ ì§ˆë¬¸ì— ëŒ€í•œ ì¤‘ë³µ API í˜¸ì¶œ ë°©ì§€)
    this.cache = new Map()
    this.cacheTimeout = 5 * 60 * 1000 // 5ë¶„

    // ìš”ì²­ ì œí•œ (Rate limiting)
    this.requestQueue = []
    this.isProcessing = false
    this.requestDelay = 1000 // 1ì´ˆ ê°„ê²©

    console.log('ğŸ¤– AI ì„œë¹„ìŠ¤ ì´ˆê¸°í™”:', this.isEnabled ? 'í™œì„±í™”ë¨' : 'ë¹„í™œì„±í™”ë¨ (API í‚¤ ì—†ìŒ)')
  }

  // API ìš”ì²­ ë˜í¼
  async makeRequest(endpoint, data, retries = 3) {
    if (!this.isEnabled) {
      throw new Error('AI ì„œë¹„ìŠ¤ê°€ í™œì„±í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”.')
    }

    const cacheKey = JSON.stringify({ endpoint, data })
    const cached = this.cache.get(cacheKey)

    // ìºì‹œëœ ì‘ë‹µì´ ìˆê³  ë§Œë£Œë˜ì§€ ì•Šì•˜ìœ¼ë©´ ë°˜í™˜
    if (cached && (Date.now() - cached.timestamp < this.cacheTimeout)) {
      console.log('ğŸ—„ï¸ AI ìºì‹œì—ì„œ ì‘ë‹µ ë°˜í™˜')
      return cached.response
    }

    for (let attempt = 1; attempt <= retries; attempt++) {
      try {
        const response = await fetch(`${this.apiUrl}${endpoint}`, {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
            'Authorization': `Bearer ${this.apiKey}`
          },
          body: JSON.stringify(data)
        })

        if (!response.ok) {
          const errorData = await response.json().catch(() => ({}))
          throw new Error(`AI API ì˜¤ë¥˜ (${response.status}): ${errorData.error?.message || response.statusText}`)
        }

        const result = await response.json()

        // ì„±ê³µí•œ ì‘ë‹µì„ ìºì‹œì— ì €ì¥
        this.cache.set(cacheKey, {
          response: result,
          timestamp: Date.now()
        })

        // ìºì‹œ í¬ê¸° ê´€ë¦¬ (ìµœëŒ€ 100ê°œ)
        if (this.cache.size > 100) {
          const firstKey = this.cache.keys().next().value
          this.cache.delete(firstKey)
        }

        return result
      } catch (error) {
        console.error(`ğŸ¤– AI API ìš”ì²­ ì‹¤íŒ¨ (ì‹œë„ ${attempt}/${retries}):`, error.message)

        if (attempt === retries) {
          throw error
        }

        // ì¬ì‹œë„ ì „ ëŒ€ê¸°
        await new Promise(resolve => setTimeout(resolve, 1000 * attempt))
      }
    }
  }

  // ì§ˆë¬¸ ë¶„ë¥˜ ë° ì¹´í…Œê³ ë¦¬ ì¶”ì²œ
  async categorizeQuestion(title, content) {
    try {
      const prompt = `
ë‹¤ìŒ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ê°€ì¥ ì ì ˆí•œ ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•˜ê³ , ê´€ë ¨ íƒœê·¸ë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”.

ì œëª©: ${title}
ë‚´ìš©: ${content}

ì‚¬ìš© ê°€ëŠ¥í•œ ì¹´í…Œê³ ë¦¬:
- ë¹„ì: ë¹„ì ì‹ ì²­, ì—°ì¥, ë³€ê²½ ë“±
- ìƒí™œì •ë³´: ì¼ìƒìƒí™œ, ì‡¼í•‘, êµí†µ ë“±
- ì·¨ì—…: êµ¬ì§, ì´ì§, ê·¼ë¡œ ì¡°ê±´ ë“±
- êµìœ¡: í•™ì—…, í•œêµ­ì–´ í•™ìŠµ, ì‹œí—˜ ë“±
- ì˜ë£Œ: ë³‘ì›, ê±´ê°•ë³´í—˜, ì˜ë£Œ ì„œë¹„ìŠ¤ ë“±
- ë²•ë¥ : ë²•ì  ë¬¸ì œ, ê³„ì•½, ê¶Œë¦¬ ë“±
- ë¬¸í™”: í•œêµ­ ë¬¸í™”, í–‰ì‚¬, ê´€ìŠµ ë“±
- ê¸°íƒ€: ìœ„ ì¹´í…Œê³ ë¦¬ì— ì†í•˜ì§€ ì•ŠëŠ” ê²½ìš°

ì‘ë‹µ í˜•ì‹ (JSON):
{
  "category": "ì¹´í…Œê³ ë¦¬ëª…",
  "confidence": 0.95,
  "tags": ["íƒœê·¸1", "íƒœê·¸2", "íƒœê·¸3"],
  "reasoning": "ë¶„ë¥˜ ì´ìœ  ì„¤ëª…"
}
`

      const data = {
        model: this.model,
        messages: [
          {
            role: 'system',
            content: 'ë‹¹ì‹ ì€ í•œêµ­ ê±°ì£¼ ë² íŠ¸ë‚¨ì¸ì„ ìœ„í•œ Q&A í”Œë«í¼ì˜ AI ë¶„ë¥˜ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ì§ˆë¬¸ì„ ì •í™•í•˜ê²Œ ë¶„ë¥˜í•˜ê³  ìœ ìš©í•œ íƒœê·¸ë¥¼ ì œì•ˆí•´ì£¼ì„¸ìš”.'
          },
          {
            role: 'user',
            content: prompt
          }
        ],
        temperature: 0.3,
        max_tokens: 500
      }

      const response = await this.makeRequest('/chat/completions', data)
      const result = response.choices[0]?.message?.content

      if (result) {
        try {
          const parsed = JSON.parse(result)
          console.log('ğŸ¤– ì§ˆë¬¸ ë¶„ë¥˜ ì™„ë£Œ:', parsed)
          return parsed
        } catch (parseError) {
          console.error('ğŸ¤– AI ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨:', parseError)
          throw new Error('AI ì‘ë‹µì„ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
        }
      }

      throw new Error('AI ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.')
    } catch (error) {
      console.error('ğŸ¤– ì§ˆë¬¸ ë¶„ë¥˜ ì‹¤íŒ¨:', error)

      // í´ë°±: ê¸°ë³¸ ë¶„ë¥˜
      return {
        category: 'ê¸°íƒ€',
        confidence: 0.1,
        tags: [],
        reasoning: 'AI ë¶„ë¥˜ ì‹¤íŒ¨ë¡œ ê¸°ë³¸ ì¹´í…Œê³ ë¦¬ ì ìš©'
      }
    }
  }

  // ì§ˆë¬¸ ê°œì„  ì œì•ˆ
  async suggestImprovements(title, content) {
    try {
      const prompt = `
ë‹¤ìŒ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ë” ì¢‹ì€ ë‹µë³€ì„ ë°›ì„ ìˆ˜ ìˆë„ë¡ ê°œì„ ì ì„ ì œì•ˆí•´ì£¼ì„¸ìš”.

ì œëª©: ${title}
ë‚´ìš©: ${content}

ë‹¤ìŒ ê´€ì ì—ì„œ ë¶„ì„í•´ì£¼ì„¸ìš”:
1. ì œëª©ì˜ ëª…í™•ì„±
2. ë‚´ìš©ì˜ êµ¬ì²´ì„±
3. í•„ìš”í•œ ì¶”ê°€ ì •ë³´
4. ì§ˆë¬¸ì˜ êµ¬ì¡°

ì‘ë‹µ í˜•ì‹ (JSON):
{
  "suggestions": [
    {
      "type": "title" | "content" | "structure" | "additional_info",
      "message": "ê°œì„  ì œì•ˆ ë‚´ìš©",
      "example": "ê°œì„  ì˜ˆì‹œ (ì˜µì…˜)"
    }
  ],
  "improvementScore": 0.7,
  "summary": "ì „ì²´ ê°œì„ ì  ìš”ì•½"
}
`

      const data = {
        model: this.model,
        messages: [
          {
            role: 'system',
            content: 'ë‹¹ì‹ ì€ ì§ˆë¬¸ ê°œì„ ì„ ë„ì™€ì£¼ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. í•œêµ­ ê±°ì£¼ ë² íŠ¸ë‚¨ì¸ë“¤ì´ ë” ì¢‹ì€ ë‹µë³€ì„ ë°›ì„ ìˆ˜ ìˆë„ë¡ ì§ˆë¬¸ì„ ê°œì„ í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•´ì£¼ì„¸ìš”.'
          },
          {
            role: 'user',
            content: prompt
          }
        ],
        temperature: 0.5,
        max_tokens: 800
      }

      const response = await this.makeRequest('/chat/completions', data)
      const result = response.choices[0]?.message?.content

      if (result) {
        try {
          const parsed = JSON.parse(result)
          console.log('ğŸ¤– ì§ˆë¬¸ ê°œì„  ì œì•ˆ ì™„ë£Œ:', parsed)
          return parsed
        } catch (parseError) {
          console.error('ğŸ¤– AI ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨:', parseError)
          return null
        }
      }

      return null
    } catch (error) {
      console.error('ğŸ¤– ì§ˆë¬¸ ê°œì„  ì œì•ˆ ì‹¤íŒ¨:', error)
      return null
    }
  }

  // ë‹µë³€ í’ˆì§ˆ í‰ê°€
  async evaluateAnswer(question, answer) {
    try {
      const prompt = `
ë‹¤ìŒ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ í‰ê°€í•˜ì—¬ ë‹µë³€ì˜ í’ˆì§ˆì„ ë¶„ì„í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: ${question}
ë‹µë³€: ${answer}

í‰ê°€ ê¸°ì¤€:
1. ì •í™•ì„±: ë‹µë³€ì´ ì§ˆë¬¸ì— ì •í™•íˆ ëŒ€ë‹µí•˜ëŠ”ê°€?
2. ì™„ì„±ë„: ë‹µë³€ì´ ì¶©ë¶„íˆ ìƒì„¸í•œê°€?
3. ë„ì›€ ì •ë„: ì§ˆë¬¸ìì—ê²Œ ì‹¤ì§ˆì  ë„ì›€ì´ ë˜ëŠ”ê°€?
4. ëª…í™•ì„±: ë‹µë³€ì´ ì´í•´í•˜ê¸° ì‰¬ìš´ê°€?
5. ì „ë¬¸ì„±: ì „ë¬¸ì  ì§€ì‹ì´ ë°˜ì˜ë˜ì—ˆëŠ”ê°€?

ì‘ë‹µ í˜•ì‹ (JSON):
{
  "score": 0.85,
  "evaluation": {
    "accuracy": 0.9,
    "completeness": 0.8,
    "helpfulness": 0.9,
    "clarity": 0.8,
    "expertise": 0.7
  },
  "strengths": ["ê°•ì 1", "ê°•ì 2"],
  "improvements": ["ê°œì„ ì 1", "ê°œì„ ì 2"],
  "summary": "ì „ì²´ í‰ê°€ ìš”ì•½"
}
`

      const data = {
        model: this.model,
        messages: [
          {
            role: 'system',
            content: 'ë‹¹ì‹ ì€ ë‹µë³€ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” AI ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í•œêµ­ ìƒí™œ ê´€ë ¨ ì§ˆë¬¸ê³¼ ë‹µë³€ì˜ í’ˆì§ˆì„ ê°ê´€ì ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”.'
          },
          {
            role: 'user',
            content: prompt
          }
        ],
        temperature: 0.3,
        max_tokens: 600
      }

      const response = await this.makeRequest('/chat/completions', data)
      const result = response.choices[0]?.message?.content

      if (result) {
        try {
          const parsed = JSON.parse(result)
          console.log('ğŸ¤– ë‹µë³€ í‰ê°€ ì™„ë£Œ:', parsed)
          return parsed
        } catch (parseError) {
          console.error('ğŸ¤– AI ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨:', parseError)
          return null
        }
      }

      return null
    } catch (error) {
      console.error('ğŸ¤– ë‹µë³€ í‰ê°€ ì‹¤íŒ¨:', error)
      return null
    }
  }

  // ê´€ë ¨ ì§ˆë¬¸ ì¶”ì²œ
  async recommendSimilarQuestions(title, content, existingQuestions = []) {
    try {
      const prompt = `
ë‹¤ìŒ ì§ˆë¬¸ê³¼ ìœ ì‚¬í•œ ì§ˆë¬¸ë“¤ì„ ê¸°ì¡´ ì§ˆë¬¸ ëª©ë¡ì—ì„œ ì°¾ì•„ ì¶”ì²œí•´ì£¼ì„¸ìš”.

í˜„ì¬ ì§ˆë¬¸:
ì œëª©: ${title}
ë‚´ìš©: ${content}

ê¸°ì¡´ ì§ˆë¬¸ë“¤:
${existingQuestions.map((q, index) =>
  `${index + 1}. [${q.category}] ${q.title}`
).join('\n')}

ì‘ë‹µ í˜•ì‹ (JSON):
{
  "recommendations": [
    {
      "index": 1,
      "title": "ì§ˆë¬¸ ì œëª©",
      "similarity": 0.8,
      "reason": "ìœ ì‚¬í•œ ì´ìœ "
    }
  ]
}
`

      const data = {
        model: this.model,
        messages: [
          {
            role: 'system',
            content: 'ë‹¹ì‹ ì€ ì§ˆë¬¸ ì¶”ì²œ ì‹œìŠ¤í…œì…ë‹ˆë‹¤. ìœ ì‚¬í•œ ì§ˆë¬¸ë“¤ì„ ì°¾ì•„ ì‚¬ìš©ìì—ê²Œ ë„ì›€ì´ ë  ë§Œí•œ ì§ˆë¬¸ë“¤ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”.'
          },
          {
            role: 'user',
            content: prompt
          }
        ],
        temperature: 0.4,
        max_tokens: 500
      }

      const response = await this.makeRequest('/chat/completions', data)
      const result = response.choices[0]?.message?.content

      if (result) {
        try {
          const parsed = JSON.parse(result)
          console.log('ğŸ¤– ê´€ë ¨ ì§ˆë¬¸ ì¶”ì²œ ì™„ë£Œ:', parsed)
          return parsed.recommendations || []
        } catch (parseError) {
          console.error('ğŸ¤– AI ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨:', parseError)
          return []
        }
      }

      return []
    } catch (error) {
      console.error('ğŸ¤– ê´€ë ¨ ì§ˆë¬¸ ì¶”ì²œ ì‹¤íŒ¨:', error)
      return []
    }
  }

  // ìë™ ë‹µë³€ ìƒì„± (ì „ë¬¸ê°€ ìŠ¹ì¸ í•„ìš”)
  async generateAutoAnswer(title, content, category) {
    try {
      const prompt = `
ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ë„ì›€ì´ ë  ë§Œí•œ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”.
ì´ ë‹µë³€ì€ ì „ë¬¸ê°€ ê²€í†  í›„ ê²Œì‹œë˜ë©°, ì •í™•í•˜ê³  ì‹¤ìš©ì ì¸ ì •ë³´ë¥¼ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.

ì œëª©: ${title}
ë‚´ìš©: ${content}
ì¹´í…Œê³ ë¦¬: ${category}

ë‹µë³€ ì‘ì„± ê°€ì´ë“œë¼ì¸:
1. ì •í™•í•œ ì •ë³´ë§Œ ì œê³µ
2. ë‹¨ê³„ë³„ë¡œ ëª…í™•í•˜ê²Œ ì„¤ëª…
3. í•„ìš”í•œ ê²½ìš° ì£¼ì˜ì‚¬í•­ ëª…ì‹œ
4. ì¶”ê°€ ë„ì›€ì„ ë°›ì„ ìˆ˜ ìˆëŠ” ê¸°ê´€ì´ë‚˜ ì—°ë½ì²˜ ì•ˆë‚´
5. í•œêµ­ì˜ ìµœì‹  ë²•ë ¹ê³¼ ì •ì±… ë°˜ì˜

ì‘ë‹µ í˜•ì‹ (JSON):
{
  "answer": "ìƒì„¸í•œ ë‹µë³€ ë‚´ìš©",
  "confidence": 0.8,
  "sources": ["ì •ë³´ ì¶œì²˜1", "ì •ë³´ ì¶œì²˜2"],
  "warnings": ["ì£¼ì˜ì‚¬í•­1", "ì£¼ì˜ì‚¬í•­2"],
  "relatedResources": [
    {
      "name": "ê¸°ê´€ëª… ë˜ëŠ” ì›¹ì‚¬ì´íŠ¸",
      "url": "ë§í¬",
      "description": "ì„¤ëª…"
    }
  ]
}
`

      const data = {
        model: this.model,
        messages: [
          {
            role: 'system',
            content: 'ë‹¹ì‹ ì€ í•œêµ­ ê±°ì£¼ ì™¸êµ­ì¸ì„ ìœ„í•œ ì „ë¬¸ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•˜ë˜, ë²•ì  ì¡°ì–¸ì´ í•„ìš”í•œ ê²½ìš° ì „ë¬¸ê°€ ìƒë‹´ì„ ê¶Œí•˜ì„¸ìš”.'
          },
          {
            role: 'user',
            content: prompt
          }
        ],
        temperature: 0.5,
        max_tokens: 1200
      }

      const response = await this.makeRequest('/chat/completions', data)
      const result = response.choices[0]?.message?.content

      if (result) {
        try {
          const parsed = JSON.parse(result)
          console.log('ğŸ¤– ìë™ ë‹µë³€ ìƒì„± ì™„ë£Œ:', parsed)
          return parsed
        } catch (parseError) {
          console.error('ğŸ¤– AI ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨:', parseError)
          return null
        }
      }

      return null
    } catch (error) {
      console.error('ğŸ¤– ìë™ ë‹µë³€ ìƒì„± ì‹¤íŒ¨:', error)
      return null
    }
  }

  // í…ìŠ¤íŠ¸ ìš”ì•½
  async summarizeText(text, maxLength = 100) {
    try {
      const prompt = `
ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ${maxLength}ì ì´ë‚´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:

${text}

ìš”ì•½ì€ í•µì‹¬ ë‚´ìš©ì„ í¬í•¨í•˜ë˜ ê°„ê²°í•´ì•¼ í•©ë‹ˆë‹¤.
`

      const data = {
        model: this.model,
        messages: [
          {
            role: 'system',
            content: 'ë‹¹ì‹ ì€ í…ìŠ¤íŠ¸ ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í•µì‹¬ ë‚´ìš©ì„ ë†“ì¹˜ì§€ ì•Šìœ¼ë©´ì„œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”.'
          },
          {
            role: 'user',
            content: prompt
          }
        ],
        temperature: 0.3,
        max_tokens: Math.min(Math.floor(maxLength * 1.5), 200)
      }

      const response = await this.makeRequest('/chat/completions', data)
      const result = response.choices[0]?.message?.content

      return result?.trim() || text.substring(0, maxLength) + '...'
    } catch (error) {
      console.error('ğŸ¤– í…ìŠ¤íŠ¸ ìš”ì•½ ì‹¤íŒ¨:', error)
      return text.substring(0, maxLength) + '...'
    }
  }

  // ì–¸ì–´ ê°ì§€ ë° ë²ˆì—­
  async detectAndTranslate(text, targetLanguage = 'ko') {
    try {
      const prompt = `
ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ ì–¸ì–´ë¥¼ ê°ì§€í•˜ê³  ${targetLanguage === 'ko' ? 'í•œêµ­ì–´' : 'ë² íŠ¸ë‚¨ì–´'}ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”:

"${text}"

ì‘ë‹µ í˜•ì‹ (JSON):
{
  "detectedLanguage": "ì–¸ì–´ì½”ë“œ",
  "confidence": 0.95,
  "translation": "ë²ˆì—­ëœ í…ìŠ¤íŠ¸",
  "originalText": "${text}"
}
`

      const data = {
        model: this.model,
        messages: [
          {
            role: 'system',
            content: 'ë‹¹ì‹ ì€ ì–¸ì–´ ê°ì§€ ë° ë²ˆì—­ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í•œêµ­ì–´ì™€ ë² íŠ¸ë‚¨ì–´ ê°„ì˜ ì •í™•í•œ ë²ˆì—­ì„ ì œê³µí•´ì£¼ì„¸ìš”.'
          },
          {
            role: 'user',
            content: prompt
          }
        ],
        temperature: 0.3,
        max_tokens: 500
      }

      const response = await this.makeRequest('/chat/completions', data)
      const result = response.choices[0]?.message?.content

      if (result) {
        try {
          const parsed = JSON.parse(result)
          console.log('ğŸ¤– ì–¸ì–´ ê°ì§€ ë° ë²ˆì—­ ì™„ë£Œ:', parsed)
          return parsed
        } catch (parseError) {
          console.error('ğŸ¤– AI ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨:', parseError)
          return null
        }
      }

      return null
    } catch (error) {
      console.error('ğŸ¤– ì–¸ì–´ ê°ì§€ ë° ë²ˆì—­ ì‹¤íŒ¨:', error)
      return null
    }
  }

  // ìºì‹œ ê´€ë¦¬
  clearCache() {
    this.cache.clear()
    console.log('ğŸ—„ï¸ AI ìºì‹œ ì •ë¦¬ ì™„ë£Œ')
  }

  getCacheStats() {
    return {
      size: this.cache.size,
      maxSize: 100,
      timeout: this.cacheTimeout
    }
  }

  // ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
  getStatus() {
    return {
      enabled: this.isEnabled,
      hasApiKey: !!this.apiKey,
      model: this.model,
      cacheSize: this.cache.size,
      queueSize: this.requestQueue.length
    }
  }
}

// ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
const aiService = new AIService()

export default aiService
export { AIService }